name: Classif model eval metrics
description: This function renders evaluation metrics for an AutoML Tabular classification
  model.
inputs:
- {name: project, type: String}
- {name: location, type: String}
- {name: api_endpoint, type: String}
- {name: thresholds_dict_str, type: String}
- {name: model, type: Model}
outputs:
- {name: metrics, type: Metrics}
- {name: metricsc, type: ClassificationMetrics}
- {name: dep_decision, type: String}
implementation:
  container:
    image: gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest
    command:
    - sh
    - -c
    - (python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1
      python3 -m pip install --quiet                 --no-warn-script-location 'google-cloud-aiplatform'
      'kfp==1.8.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
      'google-cloud-aiplatform' 'kfp==1.8.4' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef classif_model_eval_metrics(\n\
      \    project: str,\n    location: str,  # \"us-central1\",\n    api_endpoint:\
      \ str,  # \"us-central1-aiplatform.googleapis.com\",\n    thresholds_dict_str:\
      \ str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc:\
      \ Output[ClassificationMetrics],\n) -> NamedTuple(\"Outputs\", [(\"dep_decision\"\
      , str)]):  # Return parameter.\n\n    \"\"\"This function renders evaluation\
      \ metrics for an AutoML Tabular classification model.\n    It retrieves the\
      \ classification model evaluation generated by the AutoML Tabular training\n\
      \    process, does some parsing, and uses that info to render the ROC curve\
      \ and confusion matrix\n    for the model. It also uses given metrics threshold\
      \ information and compares that to the\n    evaluation results to determine\
      \ whether the model is sufficiently accurate to deploy.\n    \"\"\"\n    import\
      \ json\n    import logging\n\n     # Use the given metrics threshold(s) to determine\
      \ whether the model is \n    # accurate enough to deploy.\n    def classification_thresholds_check(metrics_dict,\
      \ thresholds_dict):\n        ## TODO: LOGIC TO DEFINE IF MODEL WILL BE DEPLOYED\
      \ 20%\n        logging.info(\"threshold checks passed.\")\n        return True\n\
      \n    thresholds_dict = json.loads(thresholds_dict_str)\n    deploy = True #classification_thresholds_check(metrics_list[0],\
      \ thresholds_dict)\n    if deploy:\n        dep_decision = \"true\"\n    else:\n\
      \        dep_decision = \"false\"\n    logging.info(\"deployment decision is\
      \ %s\", dep_decision)\n\n    return (dep_decision,)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - classif_model_eval_metrics
